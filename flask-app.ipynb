{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import yaml\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from azure.storage.blob import BlobServiceClient, BlobType\n",
    "from email_validator import validate_email, EmailNotValidError\n",
    "\n",
    "\n",
    "with open(\"api-key.yaml\", \"r\") as file:\n",
    "    data = yaml.full_load(file)\n",
    "\n",
    "# SurveyMonkey Survey\n",
    "SM_DATA = {\n",
    "    \"base_url\":f\"https://api.surveymonkey.com/v3/surveys/{data['sm']['survey-id']}\", \n",
    "    \"headers\":{\n",
    "        \"Authorization\": f\"Bearer {data['sm']['access-token']}\"\n",
    "    }, \n",
    "    \"survey-details-fp\": \"sm-survey-key.json\" # path to saved question/answer key \n",
    "}\n",
    "\n",
    "\n",
    "# CareerOneStop Skills Matcher \n",
    "COS_DATA = {\n",
    "    \"url\":f\"https://api.careeronestop.org/v1/skillsmatcher/{data['cs']['user-id']}\",\n",
    "    \"headers\":{\n",
    "        \"Authorization\": f\"Bearer {data['cs']['token-key']}\"\n",
    "    }, \n",
    "    \"survey-details-fp\": \"cos-survey-key.json\"\n",
    "}\n",
    "\n",
    "## -- Cloud storage and logging -- ## \n",
    "# This is set currently to Azure but could change \n",
    "# We would have more sophisticated logging in production, \n",
    "# e.g. use a dedicated logging service or not just uploading to a bucket, \n",
    "# use multiple logfiles in a naming system, different logfiles for type of log data,\n",
    "# logging alerts for certain kinds of log events and messages, etc.\n",
    "\n",
    "# AZ_CONNECTION_STR = data['az']['connection-str']\n",
    "# AZ_CONTAINER_NAME = data['az']['container-name']\n",
    "\n",
    "# blob_service_client = BlobServiceClient.from_connection_string(AZ_CONNECTION_STR)\n",
    "# container_client = blob_service_client.get_container_client(AZ_CONTAINER_NAME)\n",
    "log_file = \"logfile.txt\" \n",
    "\n",
    "## Any files in the script which are currently being read locally (from within app environment) might be read from cloud storage instead.\n",
    "## A cloud copy of each file should be maintained, at least. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Utility/wrapper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logging wrapper \n",
    "def log_azure(log_data, log_file=log_file):\n",
    "    print(log_data)\n",
    "    return None # dummy function placeholder for testing until logging is configured \n",
    "    ## TO-DO: Logger should automatically add timestamp to log_data by default.\n",
    "    \"\"\"Log to Azure blob -- appends to logfile if it exists already.\"\"\"\n",
    "    # Check if log_file exists in container\n",
    "    blob_client = container_client.get_blob_client(log_file)\n",
    "    if not blob_client.exists():\n",
    "        blob_client.upload_blob(log_data, blob_type=BlobType.AppendBlob)\n",
    "    else:\n",
    "        # Append the log data to the existing blob\n",
    "        blob_properties = blob_client.get_blob_properties()\n",
    "        offset = blob_properties.size\n",
    "        blob_client.upload_blob(log_data, blob_type=BlobType.AppendBlob, length=len(log_data), offset=offset)\n",
    "\n",
    "## GET request wrapper\n",
    "def request(method:str, url:str, json:dict, headers:dict, max_attempts=2):\n",
    "    \"\"\"Wrapper for request with logging and retries.\"\"\"\n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts: \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if method == \"GET\":\n",
    "                response = requests.get(url, headers=headers)\n",
    "            elif method == \"POST\":\n",
    "                response = requests.post(url, json=json, headers=headers)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            log_data = {\n",
    "                \"url\":{url}, \n",
    "                \"time\":{dt.datetime.now()}, \n",
    "                \"response_code\":{response.status_code}, \n",
    "                \"time_taken\":f\"{end_time - start_time:.2f}\"\n",
    "            }\n",
    "            log_azure(f\"INFO: {method} {log_data['url']} -- {log_data['response_code']} -- {log_data['time_taken']} -- {log_data['time']}\")\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            error_data = {\n",
    "                \"url\": url,\n",
    "                \"time\": dt.datetime.now(),\n",
    "                \"message\": str(e)\n",
    "            }\n",
    "            wait_time = random.randint(1,4)\n",
    "            log_azure(f\"ERROR: {method} {error_data['url']} -- {error_data['message']} -- {error_data['time']} -- Re-try in {wait_time} seconds.s\")\n",
    "\n",
    "            if attempts == 2: \n",
    "                raise Exception(e)\n",
    "\n",
    "            response = None\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        attempts += 1\n",
    "\n",
    "    return response \n",
    "    \n",
    "## Load JSON -- handles/logs any errors gracefully and returns None: \n",
    "def load_json(file_path:str): \n",
    "    \"\"\"Wrapper to load a JSON file and check if it exists\"\"\"\n",
    "    try: \n",
    "        with open(file_path, \"r\") as file: \n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except Exception as e: \n",
    "        log_azure(f\"ERROR loading {file_path}: {str(e)}.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "## Load to DB (TO-DO)\n",
    "def load_to_db():\n",
    "    \"\"\"Load SM responses to DB\"\"\"\n",
    "\n",
    "## Query DB \n",
    "def query_db(): \n",
    "    \"\"\"Wrapper to query DB\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Webhook Setup** (TO-DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While the SM API webhook triggers every time a response is completed, it does not include the response_id of this new response.\n",
    "\n",
    "So while the webhook can trigger our application to run every time a survey is submitted, we can't just retrieve the new survey response by its id -- we will have to retrieve all responses from SM in bulk, and filter for new responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4661653',\n",
       " 'name': 'Test Webhook',\n",
       " 'event_type': 'response_completed',\n",
       " 'subscription_url': 'https://webhook.site/7778cda8-d526-4c18-96e3-da7cf1676b11',\n",
       " 'object_type': 'survey',\n",
       " 'object_ids': ['409346397'],\n",
       " 'href': 'https://api.surveymonkey.com/v3/webhooks/4661653'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POST a webhook \n",
    "subscription_test_url = \"https://webhook.site/7778cda8-d526-4c18-96e3-da7cf1676b11\"\n",
    "sm_webhooks_url = \"https://api.surveymonkey.com/v3/webhooks\"\n",
    "header = SM_DATA['headers']\n",
    "header['Content-Type'] = 'application/json'\n",
    "json_data = {\n",
    "    \"name\": \"Test Webhook\",\n",
    "    \"subscription_url\": subscription_test_url,\n",
    "    \"event_type\": \"response_completed\",\n",
    "    \"object_type\": \"survey\",\n",
    "    \"object_ids\": [\"409346397\"] # survey id \n",
    "}\n",
    "\n",
    "response = requests.post(sm_webhooks_url, headers=header, json=json_data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SM sends HEAD request to test url, no content*\n",
    "![sm_webhook_head.png](./img/sm_webhook_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   234  100   234    0     0    639      0 --:--:-- --:--:-- --:--:--   644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [{\"id\": \"4661653\", \"name\": \"Test Webhook\", \"href\": \"https://api.surveymonkey.com/v3/webhooks/4661653\"}], \"per_page\": 50, \"page\": 1, \"total\": 1, \"links\": {\"self\": \"https://api.surveymonkey.com/v3/webhooks?per_page=50&page=1\"}}"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "curl --request GET \\\n",
    "  --url https://api.surveymonkey.com/v3/webhooks \\\n",
    "  --header 'Accept: application/json' \\\n",
    "  --header 'Authorization: Bearer MKGIdImxFYBgyHKOyrFs-HKv6giZK7pzt5Wn3gAVWGHSVSNpOjPJEtQPUsbFeseWag3x4RjSBskAfQlyqWYH898l8Dm6ML4BAE8-k-Jl1UFEwmDndCoUkAe6gE6c.NS.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100   272  100   272    0     0    764      0 --:--:-- --:--:-- --:--:--   770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"4661653\", \"name\": \"Test Webhook\", \"event_type\": \"response_completed\", \"subscription_url\": \"https://webhook.site/7778cda8-d526-4c18-96e3-da7cf1676b11\", \"object_type\": \"survey\", \"object_ids\": [\"409346397\"], \"href\": \"https://api.surveymonkey.com/v3/webhooks/4661653\"}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --request DELETE \\\n",
    "  --url https://api.surveymonkey.com/v3/webhooks/4661653 \\\n",
    "  --header 'Accept: application/json' \\\n",
    "  --header 'Authorization: Bearer MKGIdImxFYBgyHKOyrFs-HKv6giZK7pzt5Wn3gAVWGHSVSNpOjPJEtQPUsbFeseWag3x4RjSBskAfQlyqWYH898l8Dm6ML4BAE8-k-Jl1UFEwmDndCoUkAe6gE6c.NS.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get Triggered by Webhook (TO-DO)\n",
    "# def webhook_trigger() -> dict: \n",
    "#     \"\"\"Placeholder for SurveyMonkey webhook endpoint/trigger and reading payload.\"\"\"\n",
    "\n",
    "\n",
    "#     ## Example webhook callback response object: https://developer.surveymonkey.com/api/v3/#api-endpoints--webhook-callbacks\n",
    "#     placeholder_response = {\n",
    "#     \"name\": \"My Webhook\",\n",
    "#     \"filter_type\": \"collector\",\n",
    "#     \"filter_id\": \"123456789\",\n",
    "#     \"event_type\": \"response_completed\",\n",
    "#     \"event_id\": \"123456789\",\n",
    "#     \"object_type\": \"response\",\n",
    "#     \"object_id\": \"123456\",\n",
    "#     \"event_datetime\": \"2016-01-01T21:56:31.182613+00:00\",\n",
    "#     \"resources\": {\n",
    "#         \"respondent_id\": \"114409718452\", # Replaced this with Kamran's response \n",
    "#         \"recipient_id\": \"123456789\",\n",
    "#         \"collector_id\": \"123456789\",\n",
    "#         \"survey_id\": \"123456789\",\n",
    "#         \"user_id\": \"123456789\"\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     return placeholder_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://api.surveymonkey.com/v3/webhooks\"\n",
    "\n",
    "# # Define the headers\n",
    "# headers = {\n",
    "#     \"Accept\": \"application/json\",\n",
    "#     \"Authorization\": f\"Bearer {data['sm']['access-token']}\",\n",
    "#     \"Content-Type\": \"application/json\",\n",
    "# }\n",
    "\n",
    "# # Define the data payload\n",
    "# data = {\n",
    "#     \"name\": \"Response Complete Webhook\",\n",
    "#     \"subscription_url\": \"https://surveymonkey.com/webhook_receiver\",\n",
    "#     \"authorization\": \"xyz\", \n",
    "#     \"verify_ssl\": False, # not safe for production\n",
    "#     \"event_type\": \"response_completed\",\n",
    "#     \"object_type\": \"survey\",\n",
    "#     \"object_ids\": [str(data['sm']['survey-id'])]\n",
    "# }\n",
    "\n",
    "# # Send the POST request\n",
    "# response = requests.post(url, headers=headers, json=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### **Main Functions** \n",
    "\n",
    "* `get_qa_key()` - GET (or load cached copy) of question/answer key from SM or COS \n",
    "\n",
    "<br>\n",
    "\n",
    "* `combine_qa_keys()` - Combine the SM and COS question/answer keys into one combined key/translation map between the APIs\n",
    "    - Generates a refreshed map if a change is detected in the SM survey or the COS survey. \n",
    "        - The COS survey should not change at all. \n",
    "        - Changes in the SM survey should not affect its ability to match the answer keys in the COS survey.\n",
    "\n",
    "<br>\n",
    "\n",
    "* `get_sm_responses()` - GET SM survey responses \n",
    "\n",
    "<br>\n",
    "\n",
    "* `process_sm_responses()` - filter and process new SM responses from get_sm_responses()\n",
    "    - Checks against DB for already processed responses \n",
    "    - Checks if response includes valid email address (`has_valid_email()`)\n",
    "    - Checks for unexpected response ids vs. the combined Q/A key.\n",
    "    - Loads new responses into database (into 'processing' table) until they are finished\n",
    "\n",
    "<br>\n",
    "\n",
    "* `translate_post_cos()` - Use combined answer to translate the processed SM responses to COS JSON format\n",
    "    - Loads responses from processing table in\n",
    "    - Adds matching COS information to response questions/answers \n",
    "    - If response is missing a required skills-survey answer, fills with \"beginner\" \n",
    "\n",
    "<br>\n",
    "\n",
    "* `compose_email()` - Extract list of recommended jobs from COS response and compose an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET/Load question-answer keys for SurveyMonkey Survey and CareerOneStop Skills Matcher\n",
    "def get_qa_key(api=None, fetch=False) -> dict:\n",
    "    \"\"\"\n",
    "    Load list of questions/answers from either the Survey Monkey API `/details` endpoint or from CareerOneStop. \n",
    "\n",
    "    Args: \n",
    "\n",
    "    api (str):   Must be one of 'sm' (for Skills Monkey Survey) or 'cos' (for CareerOneStop)\n",
    "\n",
    "    fetch (bool):   Whether to GET new question/answer key from api or to just use locally saved copy (default is False). \n",
    "\n",
    "          If loading our cached copy fails, automatically set to True.\n",
    "\n",
    "          If fetch == True:\n",
    "            If the GET request fails, we use our cached copy.\n",
    "            If the question/answer details have changed, we update our cached copy. Any such changes may break combine_qa_keys() and this app as a whole.            \n",
    "\n",
    "        Note 500 requests/month limit to SM -- if going to use fetch option, may want to only do so periodically.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set SM vs. COS variables\n",
    "    if api == \"sm\": \n",
    "        url = f\"{SM_DATA['base_url']}/details\"\n",
    "        headers = SM_DATA['headers']\n",
    "        cached_fp = SM_DATA['survey-details-fp']\n",
    "    elif api == \"cos\":\n",
    "        url = COS_DATA['url']\n",
    "        headers = COS_DATA['headers']\n",
    "        cached_fp = COS_DATA['survey-details-fp']\n",
    "    else:\n",
    "        raise Exception(\"`api` must be one of `sm` (SurveyMonkey) or `cos` (CareerOneStop)\")\n",
    "    \n",
    "    # Load cached details \n",
    "    cached_key = load_json(cached_fp)\n",
    "    if cached_key is None: \n",
    "        log_azure(f\"WARNING: Loading {cached_fp} failed. Fetching new {api.upper()} key.\")\n",
    "        fetch = True\n",
    "\n",
    "    ## Attempt Request (if fetch == True)\n",
    "    fetched_key = None\n",
    "    if fetch: \n",
    "        try: \n",
    "            response = request(url=url, headers=headers, method=\"GET\")\n",
    "            if response.status_code != 200:\n",
    "                log_azure(f\"WARNING: GET {api.upper()} survey details -- Response Code: {response.status_code} -- Proceeding with cached file: {cached_fp}\")\n",
    "            else: \n",
    "                fetched_key = response.json()\n",
    "        except Exception as e: \n",
    "            log_azure(f\"ERROR: GET {api.upper()} survey details -- Error: {str(e)} -- Proceeding with cached file: {cached_fp}\")\n",
    "\n",
    "    ## Return Block\n",
    "    if fetched_key is None and cached_key is None:\n",
    "        raise Exception(f\"ERROR: Failed both to fetch new copy and to load cached copy of {api.upper()} Q/A key.\")  \n",
    "    elif fetched_key is None: \n",
    "        return cached_key\n",
    "    elif fetched_key != cached_key: # This block will never run so long as one of \n",
    "        log_azure(f\"WARNING: GET {api.upper()} survey details -- Fetched Q/A key conflicts with cached copy {cached_fp} -- Updating.\")           \n",
    "        with open(cached_fp, \"w\") as file: \n",
    "            json.dump(fetched_key, file)\n",
    "        return fetched_key\n",
    "    else: \n",
    "        log_azure(f\"INFO: GET {api.upper()} survey details -- Fetched Q/A key matches cached copy {cached_fp}\")\n",
    "        return fetched_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create translation map from Survey Monkey key to COS key \n",
    "def combine_qa_keys(fetch=False) -> dict: \n",
    "    \"\"\"Creates translation map from SM to COS using the question/answer keys of each.\n",
    "    \n",
    "    Args: \n",
    "\n",
    "    fetch (bool): Setting for get_qa_key() -- whether to only load local cache of question/answer keys or to fetch new copies.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ## Get question/answer keys \n",
    "    sm_key = get_qa_key(\"sm\", fetch=fetch)\n",
    "    cos_key = get_qa_key(\"cos\", fetch=fetch)\n",
    "\n",
    "    ## Prepare translation map\n",
    "    combined_map = {\n",
    "        'non-skills-matcher':[], # not to send to COS (background questions)\n",
    "        'skills-matcher':[], # to send to COS skills matcher \n",
    "    }\n",
    "\n",
    "    ## Adding relevant SM information to map\n",
    "    for p in sm_key['pages']:\n",
    "        for q in p['questions']:\n",
    "            question_type = 'skills-matcher' if \"skills matcher\" in p['title'].lower()  else \"non-skills-matcher\" \n",
    "            # ^^ page title filter fails for demo -- limited to one page (see line 35)\n",
    "            answer_ids = [d['id'] for d in q['answers']['choices']] if 'answers' in q.keys() else None\n",
    "         \n",
    "            combined_map[question_type].append({\n",
    "                'question_id':{'sm':q['id']},\n",
    "                'question_number':{'sm':q['position']},\n",
    "                'question_text':{'sm':[h['heading'] for h in q['headings']]},\n",
    "                'answer_ids':answer_ids\n",
    "            })\n",
    "\n",
    "    # Moving skills matcher/non skills matcher questions for demo, since page filter fails for demo (only one page) (TO-DO: remove when using official survey)\n",
    "    combined_map['skills-matcher'] = combined_map['non-skills-matcher'][1:]\n",
    "    combined_map['non-skills-matcher'] = [combined_map['non-skills-matcher'][0]]\n",
    "\n",
    "    ## Adding relevant COS information \n",
    "    # Check that the number of questions to send to the COS API matches the expected amount -- this would trip on test run with demo survey\n",
    "    # if len(combined_map['skills-matcher']) != len(cos_key['Skills']):\n",
    "    #     log_azure(f\"ERROR: No. of skills-matcher questions retrieved from SM {len(combined_map['skills-matcher'])} doesn't match number in COS {len(cos_key['Skills'])}\")\n",
    "    #     raise Exception\n",
    "\n",
    "    for n in range(len(combined_map['skills-matcher'])): \n",
    "        cos_q = cos_key['Skills'][n]\n",
    "        cos_answer_ids = [cos_q[\"DataPoint20\"],\n",
    "                        cos_q[\"DataPoint35\"], \n",
    "                        cos_q[\"DataPoint50\"], \n",
    "                        cos_q[\"DataPoint65\"], \n",
    "                        cos_q[\"DataPoint80\"]]\n",
    "\n",
    "        combined_map['skills-matcher'][n]['question_id']['cos'] = cos_q['ElementId']\n",
    "        combined_map['skills-matcher'][n]['question_number']['cos'] = n + 1 # correcting for 0 index\n",
    "        combined_map['skills-matcher'][n]['question_text']['cos'] = cos_q['Question']\n",
    "        combined_map['skills-matcher'][n]['answer_ids'] = dict(zip(combined_map['skills-matcher'][n]['answer_ids'], cos_answer_ids))\n",
    "\n",
    "    ## Casting question lists to dictionary for easier lookup in translation -- keeping these as lists made the previous insertion step easier\n",
    "    combined_map['skills-matcher'] = {q['question_id']['sm']:q for q in combined_map['skills-matcher']}\n",
    "    combined_map['non-skills-matcher'] = {q['question_id']['sm']:q for q in combined_map['non-skills-matcher']}\n",
    "\n",
    "    return combined_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_key = get_qa_key(api=\"sm\", fetch=False) \n",
    "# cos_key = get_qa_key(api=\"cos\", fetch=False) \n",
    "# combined_map = combine_qa_keys(fetch=False)\n",
    "\n",
    "\n",
    "# print(\"Survey Monkey Q/A Key:\".upper())\n",
    "# display(sm_key)\n",
    "# print(\"\\nCareerOneStop Q/A Key:\".upper())\n",
    "# display(cos_key)\n",
    "# print(\"\\nCombined SM/COS Q/A Key\".upper())\n",
    "# display(combined_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sm_api_response(per_page=10000, test_mode=True):\n",
    "    \"\"\"GET list of all survey responses from /surveys/{id}/responses/bulk?per_page={per_page}. \n",
    "\n",
    "    Arg: \n",
    "\n",
    "    per_pages (int): The amount of responses to retrieve per page. \n",
    "\n",
    "        - The SM API returns survey responses in pages, up to `per_page` responses per page. \n",
    "        - The SM API response begins with the *earliest* survey responses, NOT the most recent responses.\n",
    "            - If all survey responses do not fit on one page, the response object of the GET request will include a link to the page \n",
    "            of the most recent responses. See `example_multiple_pages.json` for an example, in the `links.last` attribute.\n",
    "        - Once the amount of total responses exceeds `per_page`, we will have to make a second API call to the most recent page \n",
    "        for the most recent responses.\n",
    "            - I've set `per_page=10000` as a hacky way to get around this. The function will make the second API request if this amount is exceeded\n",
    "\n",
    "    test (bool): mode to reducing API request by just loading cached copy.\n",
    "\n",
    "    \"\"\"\n",
    "    if test_mode:\n",
    "        with open(\"sm_responses.json\", \"r\") as file:\n",
    "            return json.load(file) \n",
    "\n",
    "    url =  SM_DATA['base_url'] + f\"/responses/bulk?per_page={per_page}\"\n",
    "    response = request(url, headers=SM_DATA['headers'])\n",
    "    response_json = response.json()\n",
    "\n",
    "    if 'last' in response_json['links'].keys():\n",
    "        url = response_json['links']['last']\n",
    "        log_azure(f\"WARNING: We've miraculously exceeded {per_page} survey responses. Making second request to {url}.\")\n",
    "        response = request(url, headers=SM_DATA['headers'])\n",
    "        response_json = response.json()\n",
    "\n",
    "    return response_json\n",
    "\n",
    "def has_valid_email(sm_survey_response:dict) -> bool: \n",
    "    \"\"\"Check if SM survey response includes a valid email address in the email question.\n",
    "    Use to skip a response in process_sm_responses().\n",
    "    \"\"\"\n",
    "    # TO-DO: The Survey Monkey Survey should at least validate the text format of the email address (though it doesn't support deliverability validation).\n",
    "    # Link: https://help.surveymonkey.com/en/surveymonkey/create/validating-text-fields/\n",
    "\n",
    "    # Load translation map\n",
    "    combined_map = combine_qa_keys()\n",
    "\n",
    "    # Identify the question_id of the email question in the answer key  \n",
    "    email_question_id = [q['question_id']['sm'] for q in combined_map['non-skills-matcher'].values() \n",
    "                            if 'email' in q['question_text']['sm'][0].lower()][0]\n",
    "    \n",
    "    # Check if sm_response contains the email question (if any question is ommitted, the respondent left it blank)      \n",
    "    questions = [q for p in sm_survey_response['pages'] for q in p['questions']]\n",
    "    if not any(q['id'] == email_question_id for q in questions): \n",
    "        return False \n",
    "    \n",
    "    # Validate email if one was provided \n",
    "    email_address = [q for q in questions if q['id'] == email_question_id][0]['answers'][0]['text']\n",
    "    try:\n",
    "        validate_email(email_address, check_deliverability=True)\n",
    "        return True\n",
    "    except EmailNotValidError as e:\n",
    "        log_azure(f\"WARNING: {sm_survey_response['id']} contains invalid email address: {email_address} -- {str(e)}. Skipping.\")\n",
    "        return False \n",
    "\n",
    "\n",
    "def process_sm_responses(sm_api_response) -> list[dict]: \n",
    "    \"\"\"Filter and process new SM survey responses from get_sm_api_response()\n",
    "\n",
    "\n",
    "        - Checks for unexpected question ids vs. the combined Q/A key.\n",
    "        - Checks against DB for already processed responses \n",
    "        - Checks if response includes valid email address (`has_valid_email()`)\n",
    "        - Loads (raw) new responses into database (into 'processing' table) until they are finished\n",
    "           - When these responses are successfully sent to COS and then emailed to user, they will be moved to main DB table.\n",
    "    \"\"\"\n",
    "\n",
    "    ## -- Read list of already processed responses from database (TO-DO) -- ##\n",
    "    # Database useful to check against repeated user email or IP + answers (avoid redundant emails), for retries of failures \n",
    "\n",
    "    placeholder_processed_response_ids = []\n",
    "    ## -------------------------------------------------------------------- ## \n",
    "\n",
    "    # Create/load question answer/key map \n",
    "    combined_map = combine_qa_keys()    \n",
    "\n",
    "    ## 1.) Check for unexpected question ids in new responses vs. those in COS translation map. Attempt to refresh keys if there's a mismatch.\n",
    "    # If there are still unexpected ids, there's probably an error with combine_qa_keys().\n",
    "    new_resp_question_ids = set(q['id'] for resp in sm_api_response['data'] for p in resp['pages'] for q in p['questions']\n",
    "                    if q['id'] not in placeholder_processed_response_ids)\n",
    "    refreshes = 0\n",
    "    while refreshes < 2: \n",
    "        skills_matcher_ids = set(combined_map['skills-matcher'].keys())\n",
    "        non_skills_matcher_ids = set(combined_map['non-skills-matcher'].keys())\n",
    "        expected_ids = skills_matcher_ids.union(non_skills_matcher_ids)\n",
    "        unexpected_ids = new_resp_question_ids.difference(expected_ids)\n",
    "\n",
    "        if len(unexpected_ids) > 0: \n",
    "            if refreshes == 0:  \n",
    "                log_azure(f\"WARNING: Unexpected question ids in SM responses: {unexpected_ids}. Refreshing question/answer key map.\")\n",
    "                combined_map = combine_qa_keys(fetch=True)\n",
    "            elif refreshes == 1: \n",
    "                log_azure(f\"ERROR: Unexpected question ids remain after refresh: {unexpected_ids}.\") \n",
    "                raise Exception(f\"ERROR: Unexpected question ids remain after refresh: {unexpected_ids}.\")\n",
    "            refreshes += 1\n",
    "        else: \n",
    "            break \n",
    "\n",
    "    ## 2.) Filter for new survey responses and survey responses that have valid email addresses\n",
    "    processed_responses = []\n",
    "    for resp in sm_api_response['data']: \n",
    "        if resp['id'] not in placeholder_processed_response_ids and has_valid_email(resp): \n",
    "\n",
    "            resp_dict = {\n",
    "            'response_id':resp['id'],\n",
    "            'collector_id':resp['collector_id'], \n",
    "            'questions':[] \n",
    "            }\n",
    "\n",
    "            # Add questions information\n",
    "            for p in resp['pages']:\n",
    "                for q in p['questions']:\n",
    "\n",
    "                    # Get matching question dictionary from combined_map, based on question type and SM question_id\n",
    "                    question_type = 'non-skills-matcher' if q['id'] in non_skills_matcher_ids else 'skills-matcher'\n",
    "                    q_map = combined_map[question_type][q['id']] \n",
    "\n",
    "                    # Get rest of information from q_map\n",
    "                    question_number = {'sm':q_map['question_number']['sm']}\n",
    "                    if 'cos' in q_map['question_number'].keys():\n",
    "                        question_number['cos'] = q_map['question_number']['cos']\n",
    "                    \n",
    "                    question_id = {'sm':q_map['question_id']['sm']}\n",
    "                    if 'cos' in q_map['question_id'].keys():\n",
    "                        question_id['cos'] = q_map['question_id']['cos']\n",
    "\n",
    "                    if question_type == 'skills-matcher': \n",
    "                        answers = [{'sm':a['choice_id'],\n",
    "                                    'cos':q_map['answer_ids'][a['choice_id']]} for a in q['answers']]\n",
    "                    else: \n",
    "                        answers = [{'sm':a} for a in q['answers']]\n",
    "                    # answers = []\n",
    "                    # for a in q['answers']: \n",
    "                        # if 'choice_id' in a.keys(): \n",
    "                        #     answers.append({'sm':a['choice_id']})\n",
    "                        # else: \n",
    "                        #     answers.append({'sm':a})\n",
    "\n",
    "                    resp_dict['questions'].append({'question_id':question_id, \n",
    "                                                   'question_number':question_number, \n",
    "                                                   'question_type':question_type, \n",
    "                                                   'answers':answers})\n",
    "                                                \n",
    "            processed_responses.append(resp_dict)\n",
    "\n",
    "    ## -- 3. Write (raw) new responses to processing table in DB (TO-DO)  -- ## \n",
    "    # load_to_db(...)\n",
    "    ## -------------------------------------------------------------------- ## \n",
    "\n",
    "    return processed_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_post_cos(processed_sm_responses): \n",
    "    \"\"\"Translate processed SM survey responses to COS POST objects, retrieve responses.\"\"\"\n",
    "\n",
    "    cos_data = []\n",
    "    for resp in processed_sm_responses:\n",
    "        \n",
    "        cos_request = {'SKAValueList':\n",
    "                    [{'ElementId':q['question_id']['cos'], \n",
    "                      'DataValue':str(q['answers'][0]['cos'])} for q in resp['questions'] \n",
    "                        if q['question_type'] == 'skills-matcher']}\n",
    "        \n",
    "        cos_response = request(method=\"POST\", \n",
    "                             url=COS_DATA['url'],\n",
    "                             json=cos_request, \n",
    "                             headers=COS_DATA['headers'])\n",
    "        \n",
    "        cos_data.append({'sm_response_id':resp['response_id'],\n",
    "                         'cos_request': cos_request, \n",
    "                         'cos_response':cos_response})\n",
    "\n",
    "    return cos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: POST {'https://api.careeronestop.org/v1/skillsmatcher/XjV8e71wBCteYXb'} -- {200} -- 0.49 -- {datetime.datetime(2023, 9, 29, 23, 9, 24, 377236)}\n"
     ]
    }
   ],
   "source": [
    "sm_api_response = get_sm_api_response(test_mode=True)\n",
    "processed_sm_responses = process_sm_responses(sm_api_response)\n",
    "cos_requests = translate_post_cos(processed_sm_responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
